{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob,re,os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    'airvis': pd.read_csv('restaurant/air_visit_data.csv'),\n",
    "    'airres': pd.read_csv('restaurant/air_reserve.csv'),\n",
    "    'airinfo': pd.read_csv('restaurant/air_store_info.csv'),\n",
    "    'hpgres': pd.read_csv('restaurant/hpg_reserve.csv'),\n",
    "    'hpginfo': pd.read_csv('restaurant/hpg_store_info.csv'),\n",
    "    're': pd.read_csv('restaurant/store_id_relation.csv'),\n",
    "    'sample': pd.read_csv('restaurant/sample_submission.csv'),\n",
    "    'hol': pd.read_csv('restaurant/date_info.csv')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['hpgres']=data['hpgres'].merge(data['re'],on=['hpg_store_id'],how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['hpgres'] = data['hpgres'].rename(columns={'visit_datetime': 'visit_date'})\n",
    "data['airres'] = data['airres'].rename(columns={'visit_datetime': 'visit_date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['hpgres'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for df in ['airres','hpgres']:\n",
    "    data[df]['visit_date'] = pd.to_datetime(data[df]['visit_date']).dt.date\n",
    "    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime']).dt.date\n",
    "    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (r['visit_date'] - r['reserve_datetime']).days, axis=1)\n",
    "    temp1 = data[df].groupby(['air_store_id','visit_date'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'reserve_datetime_diff': 'sum of diff', 'reserve_visitors': 'sum of res vistors'})\n",
    "    temp2 = data[df].groupby(['air_store_id','visit_date'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'reserve_datetime_diff': 'mean of diff', 'reserve_visitors':'mean of res visitors'})\n",
    "    data[df]=temp1.merge(temp2,how='inner',on=['air_store_id','visit_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['hpgres'].air_store_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['airvis']['visit_date']=pd.to_datetime(data['airvis']['visit_date'])\n",
    "data['airvis']['dow']=data['airvis']['visit_date'].dt.dayofweek\n",
    "data['airvis']['year']=data['airvis']['visit_date'].dt.year\n",
    "data['airvis']['month']=data['airvis']['visit_date'].dt.month\n",
    "data['airvis']['visit_date']=data['airvis']['visit_date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['airvis'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['sample']['visit_date'] = data['sample']['id'].map(lambda x: str(x).split('_')[2])\n",
    "data['sample']['air_store_id'] = data['sample']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "data['sample']['visit_date'] = pd.to_datetime(data['sample']['visit_date'])\n",
    "data['sample']['dow'] = data['sample']['visit_date'].dt.dayofweek\n",
    "data['sample']['year'] = data['sample']['visit_date'].dt.year\n",
    "data['sample']['month'] = data['sample']['visit_date'].dt.month\n",
    "data['sample']['visit_date'] = data['sample']['visit_date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['sample'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_stores=data['sample']['air_store_id'].unique()\n",
    "print('The number of unique stores is:', unique_stores.shape[0])\n",
    "print('total number of data records in test set is',data['sample'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stores=pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)],\n",
    "            axis=0,ignore_index=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['airvis']['id_dow']=data['airvis'].apply(lambda x: '_'.join([str(x['air_store_id']),str(x['dow'])]),axis=1)\n",
    "data['airvis']=data['airvis'].set_index('id_dow')\n",
    "data['sample']['id_dow']=data['sample'].apply(lambda x: '_'.join([str(x['air_store_id']),str(x['dow'])]),axis=1)\n",
    "data['sample']=data['sample'].set_index('id_dow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sample'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=data['airvis'].groupby(['air_store_id','dow']).agg({'visitors':[np.min, np.mean, np.median, np.max, np.size]}).reset_index()\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.columns = ['air_store_id', 'dow', 'min_visitors', 'mean_visitors', 'median_visitors','max_visitors','count_observations']\n",
    "stores=stores.merge(temp, on=['air_store_id','dow'],how='left')\n",
    "stores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores = pd.merge(stores, data['airinfo'], how='left', on=['air_store_id'])\n",
    "stores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\n",
    "stores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\n",
    "stores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import *\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for i in range(10):\n",
    "    stores['air_genre_name'+str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "    stores['air_area_name'+str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n",
    "stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n",
    "\n",
    "stores['air_genre_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['hol']['visit_date']=pd.to_datetime(data['hol']['calendar_date'])\n",
    "data['hol']['day_of_week']=lbl.fit_transform(data['hol']['day_of_week'])\n",
    "data['hol']['visit_date']=data['hol']['visit_date'].dt.date\n",
    "data['hol']=data['hol'].drop('calendar_date',axis=1)\n",
    "#merge the holiday flags to train and test sets.\n",
    "train=data['airvis'].merge(data['hol'],on=['visit_date'],how='left')\n",
    "test=data['sample'].merge(data['hol'],on=['visit_date'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=train.merge(stores,how='left',on=['air_store_id','dow'])\n",
    "test=test.merge(stores,how='left',on=['air_store_id','dow'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for df in ['airres','hpgres']:\n",
    "    train = pd.merge(train, data[df], on=['air_store_id','visit_date'], how='left')\n",
    "    test = pd.merge(test, data[df], on=['air_store_id', 'visit_date'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('train.pickle', 'wb') as f:\n",
    "    pickle.dump(train, f)\n",
    "with open('test.pickle', 'wb') as f:\n",
    "    pickle.dump(test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [f for f in train if f not in ['air_store_id', 'visit_date', 'visitors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.svm import SVR\n",
    "clf = svm.SVR()\n",
    "clf.fit(train[features], train.visitors.values) \n",
    "SVR(C=1.0, cache_size=100, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
    "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "pred = clf.predict(test[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('train.pickle', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open('test.pickle', 'rb') as f:\n",
    "    test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select C and gamma\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVR\n",
    "features = [f for f in train if f not in ['air_store_id', 'visit_date', 'visitors']]\n",
    "grid = GridSearchCV(SVR(), param_grid={\"C\":[0.1, 1, 10], \"gamma\": [1, 0.1, 0.01]}, cv=4)\n",
    "grid.fit(train[features], train.visitors.values)\n",
    "print(\"The best parameters are %s with a score of %0.2f\" %(grid.best_params_, grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.svm import SVR\n",
    "clf = svm.SVR()\n",
    "clf.fit(train[features], train.visitors.values) \n",
    "SVR(C=grid.best_params_['C'], cache_size=800, coef0=0.0, degree=3, epsilon=0.1, gamma=grid.best_params_['gamma'],\n",
    "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "pred = clf.predict(test[features])\n",
    "\n",
    "with open('predict2.pickle', 'wb') as f:\n",
    "    pickle.dump(pred, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('train.pickle', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open('test.pickle', 'rb') as f:\n",
    "    test = pickle.load(f)\n",
    "with open('predict1.pickle', 'rb') as r:\n",
    "    test['visitors'] = pickle.load(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = test[['id','visitors']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub.to_csv('sub2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
